# -*- coding: utf-8 -*-
"""mT5model_Fine_tunning(공유).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vmzaPaiTu8lBXx-fK1074G1gkmK8Y-rb

# 필요한 모듈 설치
"""

! pip install transformers datasets
! pip install rouge-score nltk
! pip install huggingface_hub
! pip install sentencepiece

from huggingface_hub import notebook_login

notebook_login()

"""# 데이터셋 만들기"""

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/문서요약/신문기사.csv')
df

df = df.dropna()

df = df[:50000]
df

from datasets import Dataset
import pyarrow as pa
import pyarrow.dataset as ds
import pandas as pd

### convert to Huggingface dataset
datasets = Dataset(pa.Table.from_pandas(df))

datasets

datasets = datasets.train_test_split(test_size=0.2)

datasets["train"][0]

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("google/mt5-small")

prefix = "summarize: "


def preprocess_function(examples):
    inputs = [prefix + doc for doc in examples["원문"]]
    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)

    with tokenizer.as_target_tokenizer():
        labels = tokenizer(examples["생성요약"], max_length=128, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

tokenized_datasets = datasets.map(preprocess_function, batched=True)

"""# 모델 불러오기"""

from transformers import TFAutoModelForSeq2SeqLM

model = TFAutoModelForSeq2SeqLM.from_pretrained('google/mt5-small')

from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors="tf")

tf_train_set = tokenized_datasets["train"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=True,
    batch_size=4,
    collate_fn=data_collator,
)

tf_test_set = tokenized_datasets["test"].to_tf_dataset(
    columns=["attention_mask", "input_ids", "labels"],
    shuffle=False,
    batch_size=4,
    collate_fn=data_collator,
)

generation_dataset = (
    tokenized_datasets["test"]
    .shuffle()
    .select(list(range(200)))
    .to_tf_dataset(
        batch_size=4,
        columns=["attention_mask", "input_ids", "labels"],
        shuffle=False,
        collate_fn=data_collator,
    )
)

"""## 평가를 위한 모듈(?) 불러오기"""

from datasets import load_metric

metric = load_metric("rouge")

fake_preds = ["hello there", "general kenobi"]
fake_labels = ["hi there", "general kenobi"]
metric.compute(predictions=fake_preds, references=fake_labels)

import numpy as np
import nltk


def metric_fn(eval_predictions):
    predictions, labels = eval_predictions
    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    for label in labels:
        label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)
    # Rouge expects a newline after each sentence
    decoded_predictions = [
        "\n".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_predictions
    ]
    decoded_labels = [
        "\n".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels
    ]
    result = metric.compute(
        predictions=decoded_predictions, references=decoded_labels, use_stemmer=True
    )
    # Extract a few results
    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}
    # Add mean generated length
    prediction_lens = [
        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions
    ]
    result["gen_len"] = np.mean(prediction_lens)

    return result

"""## hugging-face로 업로드를 위한 설정"""

model_name = 'mt5-small-finetuned-news-ab'
push_to_hub_model_id = model_name

"""## 모델 세부사항 설정 후 학습"""

from transformers import create_optimizer, AdamWeightDecay

optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)

model.compile(optimizer=optimizer)

model.summary()

from transformers.keras_callbacks import PushToHubCallback, KerasMetricCallback
from tensorflow.keras.callbacks import TensorBoard

tensorboard_callback = TensorBoard(log_dir="./summarization_model_save/logs")

push_to_hub_callback = PushToHubCallback(
    output_dir="./summarization_model_save",
    tokenizer=tokenizer,
    hub_model_id=push_to_hub_model_id,
)

metric_callback = KerasMetricCallback(
    metric_fn, eval_dataset=generation_dataset, predict_with_generate=True
)
# metric_callback은 오류나서 제외

callbacks = [tensorboard_callback, push_to_hub_callback]

# import torch
# torch.cuda.empty_cache()

# import gc
# gc.collect()

model.fit(
    tf_train_set, validation_data=tf_test_set, epochs=2, callbacks=callbacks
)

"""# 결과 확인"""

text = df['원문'][40000]

from transformers import SummarizationPipeline

# use t5 in tf
summarizer = SummarizationPipeline(model=model, tokenizer=tokenizer, framework="tf")
result = summarizer(text, min_length=5, max_length=150)
print(result)

text

"""## 평가지표
- 어떻게 하는거지?? 요약문이랑 정답이랑 비교해야지
"""

result[0]['summary_text']

